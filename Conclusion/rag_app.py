# rag_app.py
import os
import asyncio
import uuid
from pathlib import Path
from typing import List

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

import uvicorn

import streamlit as st

# langchainç›¸å…³åº“
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader

from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_deepseek import ChatDeepSeek

from dotenv import load_dotenv

# Load .env if exists
load_dotenv()

# åˆå§‹åŒ– FastAPI
app = FastAPI()

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡
if not os.getenv("DEEPSEEK_API_KEY"):
    import getpass
    os.environ["DEEPSEEK_API_KEY"] = getpass.getpass("Enter your DeepSeek API key: ")

# åˆå§‹åŒ– LLM
llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    streaming=True
)

# è§£æå™¨
parser = StrOutputParser()

# Promptæ¨¡æ¿
SYSTEM_PROMPT = """
You are a {job}, specialized in {skills}.
When answering questions:
1. First consult the provided context from the knowledge base
2. Then consider the conversation history
3. Finally provide a comprehensive answer
"""
prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

base_chain = prompt | llm | parser

# èŠå¤©å†å²å­˜å‚¨ï¼ˆå†…å­˜ä¸­ï¼‰
session_store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in session_store:
        session_store[session_id] = ChatMessageHistory()
    return session_store[session_id]

chat_chain = RunnableWithMessageHistory(
    base_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# Embeddings å’Œ å‘é‡åº“
embedder = OllamaEmbeddings(model="nomic-embed-text")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
VECTOR_DB_PATH = "./vector_db"
DATA_DIR = "./data"

class VectorDBManager:
    def __init__(self, db_path=VECTOR_DB_PATH):
        self.db_path = db_path
        self.vector_db = None
        self.load_db()

    def load_db(self):
        if os.path.exists(self.db_path):
            try:
                self.vector_db = FAISS.load_local(
                    self.db_path,
                    embedder,
                    allow_dangerous_deserialization=True
                )
                print("âœ… å·²åŠ è½½å‘é‡æ•°æ®åº“")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡åº“å¤±è´¥: {e}")
                self.vector_db = None
        else:
            self.vector_db = None

    def save_db(self):
        if self.vector_db:
            self.vector_db.save_local(self.db_path)
            print("âœ… å‘é‡åº“å·²ä¿å­˜")

    def rebuild_from_data_dir(self):
        """ä» data ç›®å½•æ‰«ææ‰€æœ‰æ”¯æŒæ–‡æ¡£ï¼Œé‡å»ºå‘é‡åº“"""
        print("ğŸ“‚ ä» data ç›®å½•é‡å»ºå‘é‡åº“...")
        docs = []
        for root, _, files in os.walk(DATA_DIR):
            for file in files:
                ext = os.path.splitext(file)[1].lower()
                path = os.path.join(root, file)
                loader = None
                try:
                    if ext == ".pdf":
                        loader = PyPDFLoader(path)
                    elif ext == ".docx":
                        loader = Docx2txtLoader(path)
                    elif ext == ".txt":
                        loader = TextLoader(path)
                    else:
                        continue  # å¿½ç•¥ä¸æ”¯æŒçš„ç±»å‹
                    loaded_docs = loader.load()
                    docs.extend(loaded_docs)
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥: {path}, é”™è¯¯: {e}")

        if not docs:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œå‘é‡åº“æœªæ›´æ–°")
            return False

        split_docs = text_splitter.split_documents(docs)

        if self.vector_db is None:
            self.vector_db = FAISS.from_documents(
                documents=split_docs,
                embedding=embedder
            )
        else:
            self.vector_db = FAISS.from_documents(
                documents=split_docs,
                embedding=embedder
            )

        self.save_db()
        print(f"âœ… å‘é‡åº“é‡å»ºå®Œæˆï¼Œæ–‡æ¡£å—æ•°: {len(split_docs)}")
        return True

    def similarity_search(self, query: str, k=3):
        if self.vector_db is None:
            return []
        return self.vector_db.similarity_search(query, k=k)


vector_db_manager = VectorDBManager()

# FastAPI è¯·æ±‚æ¨¡å‹
class RagRequest(BaseModel):
    question: str
    session_id: str = "default"

# FastAPI è·¯ç”± - ä¸Šä¼ æ–‡ä»¶æ¥å£
@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        # ä¿å­˜æ–‡ä»¶åˆ° data ç›®å½•
        Path(DATA_DIR).mkdir(parents=True, exist_ok=True)
        file_path = os.path.join(DATA_DIR, file.filename)
        with open(file_path, "wb") as f:
            f.write(await file.read())

        # é‡æ–°æ„å»ºå‘é‡åº“
        success = vector_db_manager.rebuild_from_data_dir()
        if not success:
            return {"status": "error", "message": "æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œå‘é‡åº“æœªæ›´æ–°"}

        return {"status": "success", "message": f"æ–‡ä»¶ {file.filename} ä¸Šä¼ æˆåŠŸï¼Œå‘é‡åº“å·²æ›´æ–°"}

    except Exception as e:
        return {"status": "error", "message": str(e)}

# FastAPI è·¯ç”± - é—®ç­”æ¥å£ï¼Œæµå¼è¿”å›
@app.post("/chat")
async def rag_answer(request: RagRequest):
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    docs = vector_db_manager.similarity_search(request.question, k=3)
    context = "\n\n".join([doc.page_content for doc in docs])

    async def generate_stream():
        rag_prompt = ChatPromptTemplate.from_template(
            "Context:\n{context}\n\nConversation History:\n{history}\n\nQuestion: {question}"
        )
        history = get_session_history(request.session_id)
        rag_chain = rag_prompt | llm

        async for chunk in rag_chain.astream({
            "context": context,
            "history": history.messages,
            "question": request.question
        }):
            yield str(chunk.content)
            await asyncio.sleep(0.02)

    return StreamingResponse(generate_stream(), media_type="text/event-stream")



# ==== Streamlit å‰ç«¯ ====

def streamlit_app():
    st.title("RAGé—®ç­”ç³»ç»Ÿï¼ˆFastAPI + Streamlit é›†æˆç¤ºä¾‹ï¼‰")

    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    if "messages" not in st.session_state:
        st.session_state.messages = []

    with st.sidebar:
        st.header("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (PDF, DOCX, TXT)")
        uploaded_files = st.file_uploader("é€‰æ‹©æ–‡ä»¶ä¸Šä¼ ", type=["pdf", "docx", "txt"], accept_multiple_files=True)
        if st.button("ä¸Šä¼ å¹¶æ›´æ–°å‘é‡åº“"):
            if uploaded_files:
                for f in uploaded_files:
                    files = {"file": (f.name, f.getvalue())}
                    # è°ƒç”¨ FastAPI ä¸Šä¼ æ¥å£
                    import requests
                    try:
                        response = requests.post("http://localhost:8000/upload", files=files)
                        if response.status_code == 200:
                            st.success(response.json().get("message"))
                        else:
                            st.error(f"ä¸Šä¼ å¤±è´¥: {response.text}")
                    except Exception as e:
                        st.error(f"è¯·æ±‚ä¸Šä¼ æ¥å£å¤±è´¥: {e}")
            else:
                st.warning("è¯·å…ˆé€‰æ‹©æ–‡ä»¶")

        if st.button("æ¸…ç©ºèŠå¤©å†å²"):
            st.session_state.messages = []

    # æ˜¾ç¤ºèŠå¤©è®°å½•
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.markdown(f"**ç”¨æˆ·:** {msg['content']}")
        else:
            st.markdown(f"**åŠ©æ‰‹:** {msg['content']}")

    # ç”¨æˆ·è¾“å…¥
    user_input = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š", key="input")
    if st.button("å‘é€") and user_input.strip():
        st.session_state.messages.append({"role": "user", "content": user_input})

        # è°ƒç”¨åç«¯ /chat æ¥å£è·å–å›ç­”ï¼ˆåŒæ­¥ç‰ˆç®€åŒ–ï¼‰
        import requests
        try:
            response = requests.post(
                "http://localhost:8000/chat",
                json={"question": user_input, "session_id": st.session_state.session_id},
                stream=True,
            )
            answer = ""
            if response.status_code == 200:
                for chunk in response.iter_lines(decode_unicode=True):
                    if chunk:
                        answer += chunk
                        # å®æ—¶æ˜¾ç¤ºï¼ˆç®€å•åˆ·æ–°ï¼‰
                        st.session_state.messages[-1]["content"] = user_input
                        st.experimental_rerun()
                # å®Œæ•´ç­”æ¡ˆå­˜å…¥æ¶ˆæ¯
                st.session_state.messages.append({"role": "assistant", "content": answer})
            else:
                st.error(f"æ¥å£è°ƒç”¨å¤±è´¥: {response.text}")
        except Exception as e:
            st.error(f"è°ƒç”¨åç«¯æ¥å£å¤±è´¥: {e}")



if __name__ == "__main__":
    import sys

    # ä½ å¯ä»¥é€‰æ‹©å¯åŠ¨æ–¹å¼ï¼š
    # 1. åªå¯åŠ¨ FastAPI: è¿è¡Œ `python rag_app.py api`
    # 2. åªå¯åŠ¨ Streamlit: è¿è¡Œ `python rag_app.py ui`
    # 3. åŒæ—¶å¯åŠ¨ä¸¤ä¸ªï¼ˆæ¨èåˆ†åˆ«ç”¨ä¸¤ä¸ªç»ˆç«¯ï¼‰

    if len(sys.argv) >= 2 and sys.argv[1] == "api":
        # å¯åŠ¨ FastAPI æœåŠ¡
        uvicorn.run("rag_app:app", host="0.0.0.0", port=8000, reload=True)
    elif len(sys.argv) >= 2 and sys.argv[1] == "ui":
        # å¯åŠ¨ Streamlit
        streamlit_app()
    else:
        print("ç”¨æ³•ç¤ºä¾‹ï¼š")
        print("å¯åŠ¨ API: python rag_app.py api")
        print("å¯åŠ¨ UI: python rag_app.py ui")
