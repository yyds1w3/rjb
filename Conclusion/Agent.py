# API-reading
from dotenv import load_dotenv
import os
from pathlib import Path
import getpass
import asyncio
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import AsyncGenerator, List, Union, Tuple
import uuid
# front-ending
import streamlit as st
import tempfile
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
# RAG components
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.agents import create_react_agent, AgentExecutor
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_core import prompts
from langchain_core.runnables import Runnable
from langchain_deepseek import ChatDeepSeek
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory, RedisChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredPowerPointLoader, UnstructuredHTMLLoader, UnstructuredCSVLoader,UnstructuredMarkdownLoader, UnstructuredImageLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import docx2txt
import time
from langchain.schema import Document
st.set_page_config(page_title="Agent ", layout="wide")
st.title("ğŸ“š RAG Agent(æ”¯æŒ PDF / DOCX / TXT / DOCä¸Šä¼ ä¸é—®ç­”)")
if "messages" not in st.session_state:
    st.session_state.messages = []
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "session_store" not in st.session_state:
    st.session_state.session_store = {}
# Load environment variables
load_dotenv()
if not os.getenv("DEEPSEEK_API_KEY"):
    os.environ["DEEPSEEK_API_KEY"] = getpass.getpass("Enter your DeepSeek API key: ")
# --- å¸¸é‡ ---
DATA_DIR = "./data"
VECTOR_DB_PATH = "./vector_db"
# --- åˆå§‹åŒ–ç›®å½• ---
os.makedirs(DATA_DIR, exist_ok=True)

# --- åˆå§‹åŒ– Embedding å’Œ Text Splitter ---
embedder = OllamaEmbeddings(model="nomic-embed-text")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

# --- åˆå§‹åŒ– LLM ---
llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,      # Lower temperature for more deterministic responses
    max_tokens=None,    # No limit on response length
    timeout=None,       # No timeout
    max_retries=2,      # Retry failed requests twice
    streaming=True      # Enable streaming for real-time responses
)
parser = StrOutputParser()

# System prompt template defining the AI's role and capabilities
SYSTEM_PROMPT = """
You are a {job}, specialized in {skills}.
When answering questions:
1. First consult the provided context fro.m the knowledge base
2. Then consider the conversation history
3. Finally provide a comprehensive answer
"""

# Main chat prompt combining system instructions, history and user input
prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    MessagesPlaceholder(variable_name="history"),  # Dynamic history insertion
    ("human", "{input}")                          # User input placeholder
])

# Basic chain without history
base_chain = prompt | llm | parser

# Session store for maintaining conversation history

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """Retrieve or create chat history for a given session ID"""
    if session_id not in st.session_state.session_store:
        st.session_state.session_store[session_id] = ChatMessageHistory()
    return st.session_state.session_store[session_id]

# Chain enhanced with message history capability
chat_chain = RunnableWithMessageHistory(
    base_chain,
    get_session_history,
    input_messages_key="input",      # Key for user input in the prompt
    history_messages_key="history"   # Key for history in the prompt
)
class VectorDBManager:
    def __init__(self, db_path="./vector_db"):
        self.db_path = db_path
        self.vector_db = None
        self.load_db()

    def load_db(self):
        """åŠ è½½ç°æœ‰çš„å‘é‡æ•°æ®åº“"""
        if os.path.exists(self.db_path):
            try:
                # å‘é‡åº“åŠ è½½åˆ°å†…å­˜ä¸­
                self.vector_db = FAISS.load_local(
                    self.db_path,
                    embedder,
                    allow_dangerous_deserialization=True
                )
                st.info("âœ… å·²åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“")
                print("âœ… å·²åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“")
                # å‘é‡åº“ä¿å­˜åˆ°æ–‡ä»¶é‡Œ
                self.save_db()
            except Exception as e:
                st.warning(f"âš ï¸ å‘é‡æ•°æ®åº“ä¸ºç©º: {e}")
                print(f"âš ï¸ å‘é‡æ•°æ®åº“ä¸ºç©º: {e}")
                self.vector_db = None

    def process_uploaded_file(self, file: UploadFile) -> Document:
        """åŠ è½½ä¸Šä¼ çš„æ–‡ä»¶åˆ°å¹¶è¿”å›æ–‡æ¡£å—"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶(ä¸é€‰æ‹©å­˜å…¥data)
            temp_dir = "./temp_uploads"
            os.makedirs(temp_dir, exist_ok=True)
            file_path = f"{temp_dir}/{file.name}"
            with open(file_path, "wb") as f:
                f.write(file.getbuffer())

            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
            ext = os.path.splitext(file.name)[1].lower()
            loaders = {
                '.pdf': PyPDFLoader,
                '.docx': Docx2txtLoader,
                '.doc': Docx2txtLoader,
                '.txt': TextLoader,
                '.pptx': UnstructuredPowerPointLoader,
                '.ppt': UnstructuredPowerPointLoader,
                '.html': UnstructuredHTMLLoader,
                '.htm': UnstructuredHTMLLoader,
                '.csv': UnstructuredCSVLoader,
                '.md': UnstructuredMarkdownLoader,
                '.jpg': UnstructuredImageLoader,
                '.jpeg': UnstructuredImageLoader,
                '.png': UnstructuredImageLoader
            }

            if ext not in loaders:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")

            loader = loaders[ext](file_path)
            docs = loader.load()
            splits = text_splitter.split_documents(docs)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶(ç”¨å®Œå°±å¼ƒæ‰)
            os.remove(file_path)
            return splits

        except Exception as e:
            print(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {e}")
            raise

    def add_documents(self, documents: List[Document]):
        """å‘å‘é‡æ•°æ®åº“æ·»åŠ æ–°æ–‡æ¡£"""
        if self.vector_db is None:
            self.vector_db = FAISS.from_documents(
                documents=documents,
                embedding=embedder
            )
        else:
            self.vector_db.add_documents(documents)
        self.save_db()

    def save_db(self):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        if self.vector_db:
            self.vector_db.save_local(VECTOR_DB_PATH)
            st.info(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜{VECTOR_DB_PATH}")
            print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜{VECTOR_DB_PATH}")
    def similarity_search(self, query: str, k: int = 3):
        if self.vector_db:
            return self.vector_db.similarity_search(query, k)
        return []
    def doc_count(self) -> int:
        if self.vector_db:
            # FAISSçš„index.ntotalè¡¨ç¤ºå­˜å‚¨çš„å‘é‡æ•°é‡
            return self.vector_db.index.ntotal
        return 0
# åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç®¡ç†å™¨,åŠ è½½ç°æœ‰çš„å‘é‡åº“
vector_db_manager = VectorDBManager()
class RagRequest(BaseModel):
    """Pydantic model for RAG API request"""
    question: str
    session_id: str = "default"  # Default session ID if not provided
# --- Streamlit UI ---

with st.sidebar:
    st.header("ğŸ“„ ä¸Šä¼ æ–‡æ¡£(æ”¯æŒ pdf/docx/txt)")
    uploaded_files = st.file_uploader(
    "ä¸Šä¼ æ–‡ä»¶",
    type=["pdf", "docx", "doc", "txt", "pptx", "ppt", "html", "htm", "csv", "md", "jpg", "jpeg", "png"],
    accept_multiple_files=True
)

    if st.button("ğŸ“¥ ä¸Šä¼ å¹¶é‡å»ºå‘é‡åº“"):
        if uploaded_files:
            documents = []
            for file in uploaded_files:
                save_path = os.path.join(DATA_DIR, file.name)
                documents += vector_db_manager.process_uploaded_file(file)
                with open(save_path, "wb") as f:
                    f.write(file.getbuffer())
                st.info(f"{file.name} å·²ç»ä¿å­˜è‡³ {save_path}")
            if documents:
                vector_db_manager.add_documents(documents)
                vector_db_manager.load_db()
        else:
            st.warning("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡æ¡£æ–‡ä»¶")

    if st.button("ğŸ§¹ æ¸…ç©ºèŠå¤©å†å²"):
        st.session_state.messages = []
        st.session_state.session_store = {}
        st.rerun()
    st.markdown(f"**å‘é‡æ•°æ®åº“ä¸­æ–‡æ¡£å—æ•°é‡:** {vector_db_manager.doc_count()}")

# å±•ç¤ºèŠå¤©è®°å½•
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ç”¨æˆ·è¾“å…¥é—®é¢˜(input -> markdownå¤„ç† -> message)
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ä»å‘é‡åº“æœç´¢ç›¸å…³æ–‡æ¡£
    docs = vector_db_manager.similarity_search(prompt, k=3)
    context = "\n\n".join([doc.page_content for doc in docs])
    # ç”Ÿæˆå›ç­”
    response = chat_chain.stream(
        input = {
        "job": "RAG Assistant",
        "skills": "æ–‡æ¡£ç†è§£ä¸ç»¼åˆå›ç­”",
        "input": f"Context:\n{context}\n\nQuestion: {prompt}"
    }, config={"configurable": {"session_id": st.session_state.session_id}})

    with st.chat_message("assistant"):
        placeholder = st.empty()
        response_text = ""
        for chunk in response:
            response_text += chunk
            placeholder.markdown(response_text + "â–Œ")
            time.sleep(0.02)
        placeholder.markdown(response_text)

    st.session_state.messages.append({"role": "assistant", "content": response_text})
